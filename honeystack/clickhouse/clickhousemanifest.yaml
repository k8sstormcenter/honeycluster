---
# Source: hdx-oss-v2/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: hyperdx-hdx-oss-v2-app-secrets
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  api-key: eHh4eHh4eHgteHh4eC14eHh4LXh4eHgteHh4eHh4eHh4eHh4
---
# Source: hdx-oss-v2/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: hyperdx-hdx-oss-v2-clickhouse-secrets
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  appUserPassword: aHlwZXJkeA==
  otelUserPassword: b3RlbGNvbGxlY3RvcnBhc3M=
---
# Source: hdx-oss-v2/templates/clickhouse-deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyperdx-hdx-oss-v2-clickhouse-config
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  config.xml: |-
    <?xml version="1.0"?>
    <clickhouse>
       <user_directories>
            <users_xml>
                <path>/etc/clickhouse-server/users.xml</path>
            </users_xml>
        </user_directories>
    
        <logger>
            <level>information</level>
            <console>true</console>
            <log remove="remove"/>
            <errorlog remove="remove"/>
        </logger>
    
        <listen_host>0.0.0.0</listen_host>
        <http_port>8123</http_port>
        <tcp_port>9000</tcp_port>
    
        <max_connections>4096</max_connections>
        <keep_alive_timeout>64</keep_alive_timeout>
        <max_concurrent_queries>100</max_concurrent_queries>
        <uncompressed_cache_size>8589934592</uncompressed_cache_size>
        <mark_cache_size>5368709120</mark_cache_size>
    
        <path>/var/lib/clickhouse/</path>
        <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
        <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
    
        <users_config>users.xml</users_config>
        <default_profile>default</default_profile>
        <default_database>default</default_database>
        <timezone>UTC</timezone>
        <mlock_executable>false</mlock_executable>
    
        <!-- Graceful shutdown settings -->
        <shutdown_wait_unfinished>60</shutdown_wait_unfinished>
        <!-- Prometheus exporter -->
        <prometheus>
            <endpoint>/metrics</endpoint>
            <port>9363</port>
            <metrics>true</metrics>
            <events>true</events>
            <asynchronous_metrics>true</asynchronous_metrics>
            <errors>true</errors>
        </prometheus>
    
        <!-- Query log. Used only for queries with setting log_queries = 1. -->
        <query_log>
            <database>system</database>
            <table>query_log</table>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>
    
        <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval. -->
        <metric_log>
            <database>system</database>
            <table>metric_log</table>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
            <collect_interval_milliseconds>1000</collect_interval_milliseconds>
        </metric_log>
    
        <!--
            Asynchronous metric log contains values of metrics from
            system.asynchronous_metrics.
        -->
        <asynchronous_metric_log>
            <database>system</database>
            <table>asynchronous_metric_log</table>
            <!--
                Asynchronous metrics are updated once a minute, so there is
                no need to flush more often.
            -->
            <flush_interval_milliseconds>7000</flush_interval_milliseconds>
        </asynchronous_metric_log>
    
        <!--
            OpenTelemetry log contains OpenTelemetry trace spans.
        -->
        <opentelemetry_span_log>
            <!--
                The default table creation code is insufficient, this <engine> spec
                is a workaround. There is no 'event_time' for this log, but two times,
                start and finish. It is sorted by finish time, to avoid inserting
                data too far away in the past (probably we can sometimes insert a span
                that is seconds earlier than the last span in the table, due to a race
                between several spans inserted in parallel). This gives the spans a
                global order that we can use to e.g. retry insertion into some external
                system.
            -->
            <engine>
                engine MergeTree
                partition by toYYYYMM(finish_date)
                order by (finish_date, finish_time_us, trace_id)
            </engine>
            <database>system</database>
            <table>opentelemetry_span_log</table>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </opentelemetry_span_log>
    
    
        <!-- Crash log. Stores stack traces for fatal errors.
             This table is normally empty. -->
        <crash_log>
            <database>system</database>
            <table>crash_log</table>
    
            <partition_by />
            <flush_interval_milliseconds>1000</flush_interval_milliseconds>
        </crash_log>
    
        <!-- Profiling on Processors level. -->
        <processors_profile_log>
            <database>system</database>
            <table>processors_profile_log</table>
    
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </processors_profile_log>
    
        <!-- Uncomment if use part log.
             Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).-->
        <part_log>
            <database>system</database>
            <table>part_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </part_log>
    
        <!-- Trace log. Stores stack traces collected by query profilers.
             See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->
        <trace_log>
            <database>system</database>
            <table>trace_log</table>
    
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </trace_log>
    
        <!-- Query thread log. Has information about all threads participated in query execution.
             Used only for queries with setting log_query_threads = 1. -->
        <query_thread_log>
            <database>system</database>
            <table>query_thread_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_thread_log>
    
        <!-- Query views log. Has information about all dependent views associated with a query.
             Used only for queries with setting log_query_views = 1. -->
        <query_views_log>
            <database>system</database>
            <table>query_views_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_views_log>
    
        <distributed_ddl>
            <path>/clickhouse/task_queue/ddl</path>
        </distributed_ddl>
    
        <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>
    </clickhouse>
---
# Source: hdx-oss-v2/templates/clickhouse-deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyperdx-hdx-oss-v2-clickhouse-users
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  users.xml: |-
    <?xml version="1.0"?>
    <clickhouse>
        <profiles>
            <default>
                <max_memory_usage>10000000000</max_memory_usage>
                <use_uncompressed_cache>0</use_uncompressed_cache>
                <load_balancing>in_order</load_balancing>
                <log_queries>1</log_queries>
            </default>
            <readonly>
                <readonly>2</readonly>
            </readonly>
        </profiles>
    
        <users>
            <default>
                <password></password>
                <profile>default</profile>
                <networks>
                    <ip>::1</ip>
                    <ip>127.0.0.1</ip>
                </networks>
                <quota>default</quota>
            </default>
            <app>
                <password>hyperdx</password>
                <networks>
                    <ip>10.0.0.0/8</ip>
                    <ip>172.16.0.0/12</ip>
                    <ip>192.168.0.0/16</ip>
                    <host_regexp>.*\.svc\.cluster\.local$</host_regexp>
                </networks>
                <profile>readonly</profile>
                <quota>default</quota>
                <grants>
                    <query>GRANT SHOW ON *.*</query>
                    <query>GRANT SELECT ON system.*</query>
                    <query>GRANT SELECT ON default.*</query>
                </grants>
            </app>
            <otelcollector>
                <password>otelcollectorpass</password>
                <networks>
                    <ip>10.0.0.0/8</ip>
                    <ip>172.16.0.0/12</ip>
                    <ip>192.168.0.0/16</ip>
                    <host_regexp>.*\.svc\.cluster\.local$</host_regexp>
                </networks>
                <profile>default</profile>
                <quota>default</quota>
                <grants>
                    <query>GRANT SELECT,INSERT,CREATE,SHOW ON default.*</query>
                </grants>
            </otelcollector>
        </users>
    
        <quotas>
            <default>
                <interval>
                    <duration>3600</duration>
                    <queries>0</queries>
                    <errors>0</errors>
                    <result_rows>0</result_rows>
                    <read_rows>0</read_rows>
                    <execution_time>0</execution_time>
                </interval>
            </default>
        </quotas>
    </clickhouse>
---
# Source: hdx-oss-v2/templates/configmaps/app-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyperdx-hdx-oss-v2-app-config
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  APP_PORT: "3000"
  API_PORT: "8000"
  FRONTEND_URL: "http://localhost:3000"
  HYPERDX_API_PORT: "8000"
  HYPERDX_APP_PORT: "3000"
  HYPERDX_LOG_LEVEL: "info"
  MINER_API_URL: "http://hyperdx-hdx-oss-v2-miner:5123"
  MONGO_URI: "mongodb://hyperdx-hdx-oss-v2-mongodb:27017/hyperdx"
  OTEL_SERVICE_NAME: "hdx-oss-api"
  USAGE_STATS_ENABLED: "true"
  RUN_SCHEDULED_TASKS_EXTERNALLY: "false"
  OPAMP_PORT: "4320"
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://hyperdx-hdx-oss-v2-otel-collector:4318"
---
# Source: hdx-oss-v2/templates/clickhouse-deployment.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: hyperdx-hdx-oss-v2-clickhouse-data
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 10Gi
---
# Source: hdx-oss-v2/templates/clickhouse-deployment.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: hyperdx-hdx-oss-v2-clickhouse-logs
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 5Gi
---
# Source: hdx-oss-v2/templates/mongodb-deployment.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: hyperdx-hdx-oss-v2-mongodb
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 10Gi
---
# Source: hdx-oss-v2/templates/clickhouse-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: hyperdx-hdx-oss-v2-clickhouse
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8123
      targetPort: 8123
      name: http
    - port: 9000
      targetPort: 9000
      name: native
    - port: 9363
      targetPort: 9363
      name: prometheus
  selector:
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app: clickhouse
---
# Source: hdx-oss-v2/templates/hyperdx-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hyperdx-hdx-oss-v2-app
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: 3000
      name: app
    - port: 4320
      targetPort: 4320
      name: opamp
  selector:
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app: hyperdx-hdx-oss-v2
---
# Source: hdx-oss-v2/templates/mongodb-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: hyperdx-hdx-oss-v2-mongodb
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    kubescape.io/user-defined-profile: mongo
spec:
  ports:
    - port: 27017
      targetPort: 27017
  selector:
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app: mongodb
---
# Source: hdx-oss-v2/templates/otel-collector-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: hyperdx-hdx-oss-v2-otel-collector
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    kubescape.io/user-defined-profile: otel
spec:
  ports:
    - port: 13133
      targetPort: 13133
      name: health
    - port: 24225
      targetPort: 24225
      name: fluentd
    - port: 4317
      targetPort: 4317
      name: otlp-grpc
    - port: 4318
      targetPort: 4318
      name: otlp-http
    - port: 8888
      targetPort: 8888
      name: metrics
  selector:
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app: otel-collector
---
# Source: hdx-oss-v2/templates/clickhouse-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hyperdx-hdx-oss-v2-clickhouse
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    kubescape.io/user-defined-profile: clickhouse
    app: clickhouse
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: hdx-oss-v2
      app.kubernetes.io/instance: hyperdx
      app: clickhouse
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hdx-oss-v2
        app.kubernetes.io/instance: hyperdx
        app: clickhouse
        kubescape.io/user-defined-profile: clickhouse
    spec:
      terminationGracePeriodSeconds: 90
      containers:
        - name: clickhouse
          image: "clickhouse/clickhouse-server:25.7-alpine"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8123
            - containerPort: 9000
          env:
            - name: CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT
              value: "1"
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    clickhouse-client --query "SYSTEM STOP MERGES" || true
                    clickhouse-client --query "SYSTEM STOP MOVES" || true
                    clickhouse-client --query "SYSTEM FLUSH LOGS" || true
                    sleep 5
          livenessProbe:
            httpGet:
              path: /ping
              port: 8123
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /ping
              port: 8123
            initialDelaySeconds: 1
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /ping
              port: 8123
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30
          volumeMounts:
            - name: config
              mountPath: /etc/clickhouse-server/config.xml
              subPath: config.xml
            - name: users
              mountPath: /etc/clickhouse-server/users.xml
              subPath: users.xml
            - name: data
              mountPath: /var/lib/clickhouse
            - name: logs
              mountPath: /var/log/clickhouse-server
      volumes:
        - name: config
          configMap:
            name: hyperdx-hdx-oss-v2-clickhouse-config
        - name: users
          configMap:
            name: hyperdx-hdx-oss-v2-clickhouse-users
        - name: data
          persistentVolumeClaim:
            claimName: hyperdx-hdx-oss-v2-clickhouse-data
        - name: logs
          persistentVolumeClaim:
            claimName: hyperdx-hdx-oss-v2-clickhouse-logs
---
# Source: hdx-oss-v2/templates/hyperdx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hyperdx-hdx-oss-v2-app
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    app: hyperdx-hdx-oss-v2
    kubescape.io/user-defined-profile: app
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: hdx-oss-v2
      app.kubernetes.io/instance: hyperdx
      app: hyperdx-hdx-oss-v2
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hdx-oss-v2
        app.kubernetes.io/instance: hyperdx
        app: hyperdx-hdx-oss-v2
        kubescape.io/user-defined-profile: app
      annotations:
    spec:
      initContainers:
        - name: wait-for-mongodb
          image: "busybox@sha256:1fcf5df59121b92d61e066df1788e8df0cc35623f5d62d9679a41e163b6a0cdb"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until nc -z hyperdx-hdx-oss-v2-mongodb 27017; do echo waiting for mongodb; sleep 2; done;']
      containers:
        - name: app
          image: "docker.hyperdx.io/hyperdx/hyperdx:2.6.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: app-port
              containerPort: 3000
            - name: api-port
              containerPort: 8000
            - name: opamp-port
              containerPort: 4320
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 1
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          envFrom:
            - configMapRef:
                name: hyperdx-hdx-oss-v2-app-config
          env:
            - name: HYPERDX_API_KEY
              valueFrom:
                secretKeyRef:
                  name: hyperdx-hdx-oss-v2-app-secrets
                  key: api-key
            - name: DEFAULT_CONNECTIONS
              value: "[\n  {\n    \"name\": \"Local ClickHouse\",\n    \"host\": \"http://hyperdx-hdx-oss-v2-clickhouse:8123\",\n    \"port\": 8123,\n    \"username\": \"app\",\n    \"password\": \"hyperdx\"\n  }\n]\n"
            - name: DEFAULT_SOURCES
              value: "[\n  {\n    \"from\": {\n      \"databaseName\": \"default\",\n      \"tableName\": \"otel_logs\"\n    },\n    \"kind\": \"log\",\n    \"timestampValueExpression\": \"TimestampTime\",\n    \"name\": \"Logs\",\n    \"displayedTimestampValueExpression\": \"Timestamp\",\n    \"implicitColumnExpression\": \"Body\",\n    \"serviceNameExpression\": \"ServiceName\",\n    \"bodyExpression\": \"Body\",\n    \"eventAttributesExpression\": \"LogAttributes\",\n    \"resourceAttributesExpression\": \"ResourceAttributes\",\n    \"defaultTableSelectExpression\": \"Timestamp,ServiceName,SeverityText,Body\",\n    \"severityTextExpression\": \"SeverityText\",\n    \"traceIdExpression\": \"TraceId\",\n    \"spanIdExpression\": \"SpanId\",\n    \"connection\": \"Local ClickHouse\",\n    \"traceSourceId\": \"Traces\",\n    \"sessionSourceId\": \"Sessions\",\n    \"metricSourceId\": \"Metrics\"\n  },\n  {\n    \"from\": {\n      \"databaseName\": \"default\",\n      \"tableName\": \"otel_traces\"\n    },\n    \"kind\": \"trace\",\n    \"timestampValueExpression\": \"Timestamp\",\n    \"name\": \"Traces\",\n    \"displayedTimestampValueExpression\": \"Timestamp\",\n    \"implicitColumnExpression\": \"SpanName\",\n    \"serviceNameExpression\": \"ServiceName\",\n    \"bodyExpression\": \"SpanName\",\n    \"eventAttributesExpression\": \"SpanAttributes\",\n    \"resourceAttributesExpression\": \"ResourceAttributes\",\n    \"defaultTableSelectExpression\": \"Timestamp,ServiceName,StatusCode,round(Duration/1e6),SpanName\",\n    \"traceIdExpression\": \"TraceId\",\n    \"spanIdExpression\": \"SpanId\",\n    \"durationExpression\": \"Duration\",\n    \"durationPrecision\": 9,\n    \"parentSpanIdExpression\": \"ParentSpanId\",\n    \"spanNameExpression\": \"SpanName\",\n    \"spanKindExpression\": \"SpanKind\",\n    \"statusCodeExpression\": \"StatusCode\",\n    \"statusMessageExpression\": \"StatusMessage\",\n    \"connection\": \"Local ClickHouse\",\n    \"logSourceId\": \"Logs\",\n    \"sessionSourceId\": \"Sessions\",\n    \"metricSourceId\": \"Metrics\"\n  },\n  {\n    \"from\": {\n      \"databaseName\": \"default\",\n      \"tableName\": \"\"\n    },\n    \"kind\": \"metric\",\n    \"timestampValueExpression\": \"TimeUnix\",\n    \"name\": \"Metrics\",\n    \"resourceAttributesExpression\": \"ResourceAttributes\",\n    \"metricTables\": {\n      \"gauge\": \"otel_metrics_gauge\",\n      \"histogram\": \"otel_metrics_histogram\",\n      \"sum\": \"otel_metrics_sum\",\n      \"_id\": \"682586a8b1f81924e628e808\",\n      \"id\": \"682586a8b1f81924e628e808\"\n    },\n    \"connection\": \"Local ClickHouse\",\n    \"logSourceId\": \"Logs\",\n    \"traceSourceId\": \"Traces\",\n    \"sessionSourceId\": \"Sessions\"\n  },\n  {\n    \"from\": {\n      \"databaseName\": \"default\",\n      \"tableName\": \"hyperdx_sessions\"\n    },\n    \"kind\": \"session\",\n    \"timestampValueExpression\": \"TimestampTime\",\n    \"name\": \"Sessions\",\n    \"displayedTimestampValueExpression\": \"Timestamp\",\n    \"implicitColumnExpression\": \"Body\",\n    \"serviceNameExpression\": \"ServiceName\",\n    \"bodyExpression\": \"Body\",\n    \"eventAttributesExpression\": \"LogAttributes\",\n    \"resourceAttributesExpression\": \"ResourceAttributes\",\n    \"defaultTableSelectExpression\": \"Timestamp,ServiceName,SeverityText,Body\",\n    \"severityTextExpression\": \"SeverityText\",\n    \"traceIdExpression\": \"TraceId\",\n    \"spanIdExpression\": \"SpanId\",\n    \"connection\": \"Local ClickHouse\",\n    \"logSourceId\": \"Logs\",\n    \"traceSourceId\": \"Traces\",\n    \"metricSourceId\": \"Metrics\"\n  }\n]\n"
---
# Source: hdx-oss-v2/templates/mongodb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hyperdx-hdx-oss-v2-mongodb
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    kubescape.io/user-defined-profile: mongo
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: hdx-oss-v2
      app.kubernetes.io/instance: hyperdx
      app: mongodb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hdx-oss-v2
        app.kubernetes.io/instance: hyperdx
        kubescape.io/user-defined-profile: mongo
        app: mongodb
    spec:
      containers:
        - name: mongodb
          image: "mongo:5.0.14-focal"
          ports:
            - containerPort: 27017
          livenessProbe:
            tcpSocket:
              port: 27017
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            tcpSocket:
              port: 27017
            initialDelaySeconds: 1
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          volumeMounts:
            - name: mongodb-data
              mountPath: /data/db
      volumes:
        - name: mongodb-data
          persistentVolumeClaim:
            claimName: hyperdx-hdx-oss-v2-mongodb
---
# Source: hdx-oss-v2/templates/otel-collector-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hyperdx-hdx-oss-v2-otel-collector
  labels:
    helm.sh/chart: hdx-oss-v2-0.8.0
    app.kubernetes.io/name: hdx-oss-v2
    app.kubernetes.io/instance: hyperdx
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    kubescape.io/user-defined-profile: otel
    app: otel-collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: hdx-oss-v2
      app.kubernetes.io/instance: hyperdx
      app: otel-collector
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hdx-oss-v2
        app.kubernetes.io/instance: hyperdx
        kubescape.io/user-defined-profile: otel
        app: otel-collector
      annotations:
    spec:
      containers:
        - name: otel-collector
          image: "docker.hyperdx.io/hyperdx/hyperdx-otel-collector:2.6.0"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 13133
            - containerPort: 24225
            - containerPort: 4317
            - containerPort: 4318
            - containerPort: 8888
          livenessProbe:
            httpGet:
              path: /
              port: 13133
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: 13133
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          env:
            - name: CLICKHOUSE_ENDPOINT
              value: "tcp://hyperdx-hdx-oss-v2-clickhouse:9000?dial_timeout=10s"
            - name: CLICKHOUSE_SERVER_ENDPOINT
              value: "hyperdx-hdx-oss-v2-clickhouse:9000"
            - name: CLICKHOUSE_PROMETHEUS_METRICS_ENDPOINT
              value: "hyperdx-hdx-oss-v2-clickhouse:9363"
            - name: OPAMP_SERVER_URL
              value: http://hyperdx-hdx-oss-v2-app:4320
            - name: HYPERDX_LOG_LEVEL
              value: info
            - name: HYPERDX_OTEL_EXPORTER_CLICKHOUSE_DATABASE
              value: default
            - name: HYPERDX_API_KEY
              valueFrom:
                secretKeyRef:
                  name: hyperdx-hdx-oss-v2-app-secrets
                  key: api-key
            - name: CLICKHOUSE_USER
              value: otelcollector
            - name: CLICKHOUSE_PASSWORD
              value: otelcollectorpass

